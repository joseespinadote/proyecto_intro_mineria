{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hito 3: Corona virus (Covid-19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presentación\n",
    "\n",
    "Somos el equipo conformado por Raúl Cid, José Espina, Michelle Valenzuela y Alejandro Veragua. Nuestro *dataset* corresponde al [publicado en el portal Kaggle](https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset) construido y mantenido por la Universidad John Hopkins, al igual que para el hito 1\n",
    "\n",
    "## Comentarios sobre el hito 1\n",
    "\n",
    "Se realizó un análisis exploratorio, donde descubrimos varios datos inconsistentes y outliers\n",
    "Se buscaron correlaciones contra atributos como el ingreso per cápita, la temperatura, el índice de pobreza de los diferentes países, pero sin éxito. Finalmente se realizó una prueba de concepto de clústering a partir de los coeficientes de un modelo de regeresión polinomial donde sí aparecieron resultados interesantes\n",
    "\n",
    "## Propuesta del hito 3\n",
    "Considerando que los polinomios hacen \"overfitting\" cuando el grado es muy alto, como el ejercicio realizado en el hito 1, aprovecharemos hacer algo similar, con la diferencia de usar un modelo auto-regresivo y hacer clustering sobre los coeficientes que resulten del modelo. En particular, quisimos experimentar con un modelo auto-regresivo con media móvil (ARMA). Como se tienen datos sólo de los últimos meses y el virus es nuevo, los datos no son estacionarios (o no hay suficientes datos para demostrar que lo son), por lo que usaremos una variante llamada ARIMA (donde la \"i\" hace referencia a la integración), que permite encontrar las diferencias no-estacionarias para lograr la estacionariedad (https://www.statisticshowto.com/arma-model/)\n",
    "\n",
    "## Metodología de trabajo para el hito 3\n",
    "* se preprocesaron los datos, removiendo los datos outliers, ya que descubrimos en el hito 1 que hay paises con datos demasiado alejados de la tendencia central\n",
    "\n",
    "Además, se refuerza la idea de 2 artículos publicados que usan el mismo acercamiento\n",
    "1) Benvenuto, D., Giovanetti, M., Vassallo, L., Angeletti, S., & Ciccozzi, M. (2020). Application of the ARIMA model on the COVID-2019 epidemic dataset. Data in brief, 105340.\n",
    "2) Yang, Q., Wang, J., Ma, H., & Wang, X. (2020). Research on COVID-19 Based on ARIMA ModelΔ—Taking Hubei, China as an example to see the epidemic in Italy. Journal of Infection and Public Health.\n",
    "\n",
    "NOTA IMPORTANTE: Para ejecutar este notebook, se necesitan scikit, matplotlib, numpy y statsmodels. Éste último para poder aplicar el modelo ARIMA. Se debe instalar con pip en el ambiente del libro:\n",
    "\n",
    "$ pip install statsmodels\n",
    "\n",
    "o con conda:\n",
    "\n",
    "$ conda install -c conda-forge statsmodels\n",
    "\n",
    "(https://www.statsmodels.org/stable/install.html)\n",
    "\n",
    "NOTA IMPORTANTE 2: Calcular el modelo ARIMA toma un tiempo de cómputo importante\n",
    "\n",
    "En detalle, el plan es:\n",
    "\n",
    "* Calcular ARIMA para todos los países\n",
    "\n",
    "\n",
    "## Configuración base del notebook\n",
    "El bloque a continuación configura el libro, definiendo funciones y cargando las fuentes de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Librería de álgebra lineal\n",
    "import numpy as np \n",
    "# Procesamiento de datos y carga de archivos CSV\n",
    "import pandas as pd\n",
    "# Librearía gráfica\n",
    "import matplotlib.pylab as plt\n",
    "# Librería para usar recursos del sistema operativo\n",
    "import os \n",
    "# Librearía usada para extraer el nombre del archivo de cada path\n",
    "import ntpath\n",
    "# Librería gráfica\n",
    "import seaborn as sns\n",
    "# Librearía para usar herramientas relacionadas con\n",
    "# aprendizaje máquina. Se usó para preprocesar y\n",
    "# calcular regresión\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "# Colores\n",
    "from matplotlib import colors\n",
    "# Ticker\n",
    "import matplotlib.ticker as ticker\n",
    "# Animation\n",
    "import matplotlib.animation as animation\n",
    "# Arima\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Configuración general\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Matplotlib inline\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Se prepara carga de los archivos del dataset\n",
    "paths = []\n",
    "for dirname, _, filenames in os.walk('./novel-corona-virus-2019-dataset'):\n",
    "    for filename in filenames:\n",
    "        path = os.path.join(dirname, filename)\n",
    "        paths.append(path);\n",
    "dataFrames = {}\n",
    "for path in paths:\n",
    "    dataFrames[ntpath.basename(path)] = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preproceso: Outliers\n",
    "Como parte del preprocesamiento, se realiza el siguiente tratamiento a los datos: Para cada país, se revisan las columnas de confirmados, fallecidos y recuperados. En cada una de ellas se buscan outliers, en este caso, valores que superen en magnitud al percentil 99 o que sean menores a cero. Estos se transorman a NaN y luego interpolamos los datos para llenar estos espacios.\n",
    "Además, se debe recalcular la cantidad acumulada por día para cada columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dset_base = pd.read_csv('dataset.csv')\n",
    "for pais in dset_base['pais'].unique():\n",
    "    for col in ['num_confirmados', 'num_fallecidos', 'num_recuperados']:\n",
    "        pais_col = dset_base[dset_base['pais']==str(pais)][col]\n",
    "        changed_values = pd.Series(np.nan, index=pais_col.index)\n",
    "        for value in pais_col.iteritems():\n",
    "            if value[1] > pais_col.quantile(.99) or value[1] < 0:\n",
    "                changed_values[value[0]] = np.nan\n",
    "            else:\n",
    "                changed_values[value[0]] = value[1]\n",
    "        changed_values.interpolate(inplace=True)\n",
    "        acc = changed_values.cumsum()\n",
    "        dset_base.loc[(dset_base['pais']==pais), col] = changed_values\n",
    "        dset_base.loc[(dset_base['pais']==pais), str('acc') + col[3:]] = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo ARIMA\n",
    "Usamos un modelo ARIMA p=5,d=1,q=0. El detalle de estos parámetros se explica a continuación (basado en https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/)\n",
    "\n",
    "* El parámetro \"p\" es el número de observaciones pasadas que considera el modelo. También se le llama \"orden\". Usamos  14 debido a que es el tiempo que demora una persona es saber que tiene el virus, considerando tiempos de síntomas y resultado del exámen. Además, 14 días es el tiempo que dura la cuarentena para las personas con resultado positivo en su exámen\n",
    "* El parámetro \"d\" es el grado de diferenciación. Esto se usa para generar estacionariedad en series no estacionarias\n",
    "* El parámetro \"q\" es el tamaño de la media móvil para el error de los datos anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se calculan coeficientes del ARIMA por país. Si ya está calculado se lee el csv\n",
    "dframe_arima = None\n",
    "if os.path.isfile('arima_coefs.csv') :\n",
    "    dframe_arima = pd.read_csv('arima_coefs.csv')\n",
    "else :\n",
    "    pais_arima_coef = dict()\n",
    "    paises_error = []\n",
    "    for pais in dset_base.pais.unique() :\n",
    "        dbset_actual = dset_base[dset_base['pais']==pais][['num_dia_desde_primer_caso','num_confirmados']]\n",
    "        dbset_actual= dbset_actual.reset_index()\n",
    "        series_actual = pd.Series(dbset_actual.num_confirmados.values, index=dbset_actual.num_dia_desde_primer_caso)\n",
    "        series_actual.index = pd.DatetimeIndex(series_actual.index).to_period('D')\n",
    "        try :\n",
    "            model = ARIMA(series_actual.astype(float), order=(5,1,0),missing=\"drop\")\n",
    "            model_fit = model.fit(disp=False)\n",
    "            pais_arima_coef[pais] = model_fit.params\n",
    "        except :\n",
    "            paises_error.append(pais)\n",
    "    dframe_arima = pd.DataFrame(data=pais_arima_coef)\n",
    "    dframe_arima.to_csv('arima_coefs.csv')\n",
    "print(dframe_arima)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el fin de hacer una revision a los resultados del clustering se procede a realizar un analisis de las tendencias de las  caracteristicas de cada uno de los clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = None\n",
    "values= np.unique(clustering)\n",
    "#ex: clustering= agglomerative_clusters \n",
    "values= np.unique(agglomerative_clusters)\n",
    "\n",
    "data_agl = features_df.copy()\n",
    "data_agl['cluster_num'] = agglomerative_clusters #### \n",
    "data_agl = data_agl[['region', 'pais', 'cluster_agg']]\n",
    "data_agl_comp = pd.merge(data_agl, dset_extra, how='inner')\n",
    "data_agl_comp = dset_base.merge(data_agl_comp.set_index('pais'), on='pais') #variable sera renombrada\n",
    "# de esto lo que importa es la ultima linea (que es la data a trabajar), data_clusters contiene todos los atributos:\n",
    "data_clusters = 'la data que deberia tener uwu'\n",
    "\n",
    "datas={}\n",
    "for i in range(0, len(values)): #separamos y guardamos en diccionario\n",
    "    llave=values[i]\n",
    "    datai = data_clusters.loc[data_clusters['cluster_num']==values[i]]\n",
    "    datas[llave]=datai\n",
    "\n",
    "interes=[ 'gdp_rank', 'gdp_usd', 'gdp_en_salud', 'edad_media', 'gini', 'poblacion', 'taza_muerte1000' ]\n",
    "colores= ['darkorange', 'springgreen', 'lightseagreen', 'orchid', 'chocolate', 'lightsteelblue', 'indianred', 'cadetblue', 'gold']\n",
    "\n",
    "#estadisticos generales\n",
    "for i in range(0,len(values)):\n",
    "    print('info gnral cluster ', values[i], ':')\n",
    "    for a in range(0,len(interes)):\n",
    "        datos=datas[values[i]]\n",
    "        print(interes[a], 'promedio: ', datos[interes[a]].mean(), ';  desviacion:   ', datos[interes[a]].std() )\n",
    "        print('---------------------------------------')\n",
    "    print('*******************', 'fin cluster <', values[i] ,'>************************')\n",
    "\n",
    "\n",
    "#paises en el cluster\n",
    "for i in range(0,len(values)):\n",
    "    print('cluster', values[i])\n",
    "    data=datas[values[i]]\n",
    "    column_valxs = data[[\"pais\"]].values\n",
    "    unique_values =  np.unique(column_valxs)    \n",
    "    print(unique_values)\n",
    "    print('-----------------------------------------------------------------------------')\n",
    "\n",
    "#boxplots\n",
    "for col in interes:\n",
    "    par=col\n",
    "    data_clusters.groupby('cluster_num')\n",
    "    f, ax = plt.subplots(figsize=(7,5))\n",
    "    img=sns.boxplot(y=par, x= 'cluster_num', data=data_clusters, width=0.3, showfliers = True) ##showf en false no muestra outL\n",
    "\n",
    "#densidades (permite apreciar mejor las tendencias)\n",
    "for col in interes:\n",
    "    par=col\n",
    "    plt.figure()\n",
    "    plt.title(col)\n",
    "    for i in range(0,len(datas)):\n",
    "        datos=datas[i]\n",
    "        datos=list(datos[col])\n",
    "        sns.distplot(datos, hist=False, color=colores[i])\n",
    "    plt.close\n",
    "\n",
    "#Matrices de correlacion\n",
    "for i in range(0,len(values)):\n",
    "    print('para cluster:  ', values[i])\n",
    "    data=datas[values[i]]\n",
    "    mm_scaler = preprocessing.MinMaxScaler()\n",
    "    data = data.drop([\"fecha_observacion\"  ,\"region_x\", \"region_y\",\"cluster_num\", \"pais\", \"num_dia_desde_22_enero\", \"num_dia_desde_primer_caso\" ],axis=1)\n",
    "    #quiza eso se pueda cambiar, lo de arriba\n",
    "    scaled_at = mm_scaler.fit_transform(data)\n",
    "    scaled = pd.DataFrame(scaled_at)\n",
    "    cormatrix = scaled.corr()\n",
    "    f, ax = plt.subplots()\n",
    "    sns.heatmap(cormatrix,vmax=1,vmin=-1,center=0, cmap='rainbow' )\n",
    "\n",
    "#Algunas curvas en el cluster:\n",
    "for i in values:\n",
    "    print('llave: ', i)\n",
    "    dato=datas[i]\n",
    "    column_valxs = dato[[\"pais\"]].values\n",
    "    muestra = random.choices(np.unique(column_valxs), k=7)\n",
    "    k=0\n",
    "    var='num_dia_desde_primer_caso'\n",
    "    plt.figure()\n",
    "    plt.title('cluster'+ str(i))\n",
    "    for k in range(0,len(muestra)):\n",
    "        pais=muestra[k]\n",
    "        print('pais: ', pais)\n",
    "        datos=data_clusters.loc[data_clusters['pais']==pais]\n",
    "        sns.lineplot(x=var, y='acc_confirmados', data=datos, color=colores[k])\n",
    "    plt.close  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discusion y conclusiones**\n",
    "\n",
    "-Respecto al clustering:\n",
    "\n",
    "Las feautures entregadas por el modelo AR parecieran no ser de mucha utilidad para realizar clustering. Como se observa en el analisis de caracteristicas de cada cluster, no existen evidencias o indicios suficientes para asegurar la coherencia del agrupamiento. No se observan patrones o tendencias claras respecto del tipo de curva que se describe, asimismo tampoco existen resultados significativos en cuanto a los indicadores caracteristicos de paises.\n",
    "\n",
    "-Respecto al modelo:\n",
    "\n",
    "-Respecto a los datos:\n",
    "\n",
    "Como se puede observar en este informe, fue necesario realizar una gran cantidad de pre-procesamiento y limpieza a los datos debido a que existian una gran cantidad de inconsistencias (ruido) generado por la forma en que los paises han estado recopilando la informacion. Por ejemplo se tiene el caso de Chile donde se podia apreciar un peak en la columna de casos confirmados (saltaba de 5000 a 35000 aprox), este pico se debia a 'casos pendientes' que el minsal nohabia reportado. Para varios paises y otras columnas existian problemas de indole similar. \n",
    "Fue de gran relevancia la limpieza de los datos debido a que podian generarse clusters que agrupasen estos ruidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Direcciones futuras:**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
