{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hito 3: Corona virus (Covid-19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presentación\n",
    "\n",
    "Somos el equipo conformado por Raúl Cid, José Espina, Michelle Valenzuela y Alejandro Veragua. Nuestro *dataset* corresponde al [publicado en el portal Kaggle](https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset) construido y mantenido por la Universidad John Hopkins, al igual que para el hito 1\n",
    "\n",
    "## Comentarios sobre el hito 1\n",
    "\n",
    "Se realizó un análisis exploratorio, donde descubrimos varios datos inconsistentes y outliers\n",
    "Se buscaron correlaciones contra atributos como el ingreso per cápita, la temperatura, el índice de pobreza de los diferentes países, pero sin éxito. Finalmente se realizó una prueba de concepto de clústering a partir de los coeficientes de un modelo de regeresión polinomial donde sí aparecieron resultados interesantes\n",
    "\n",
    "## Propuesta del hito 3\n",
    "Considerando que los polinomios hacen \"overfitting\" cuando el grado es muy alto, como el ejercicio realizado en el hito 1, aprovecharemos hacer algo similar, con la diferencia de usar un modelo auto-regresivo y hacer clustering sobre los coeficientes que resulten del modelo. En particular, quisimos experimentar con un modelo auto-regresivo con media móvil (ARMA). Como se tienen datos sólo de los últimos meses y el virus es nuevo, los datos no son estacionarios (o no hay suficientes datos para demostrar que lo son), por lo que usaremos una variante llamada ARIMA (donde la \"i\" hace referencia a la integración), que permite encontrar las diferencias no-estacionarias para lograr la estacionariedad (https://www.statisticshowto.com/arma-model/)\n",
    "\n",
    "Además, se refuerza la idea de usar el modelo ARIMA ya que lo usan en 2 artículos publicados en la literatura:\n",
    "1) Benvenuto, D., Giovanetti, M., Vassallo, L., Angeletti, S., & Ciccozzi, M. (2020). Application of the ARIMA model on the COVID-2019 epidemic dataset. Data in brief, 105340.\n",
    "2) Yang, Q., Wang, J., Ma, H., & Wang, X. (2020). Research on COVID-19 Based on ARIMA ModelΔ—Taking Hubei, China as an example to see the epidemic in Italy. Journal of Infection and Public Health.\n",
    "\n",
    "## Metodología de trabajo para el hito 3\n",
    "Los siguientes pasos describen, a grandes rasgos, nuestra metodología planificada:\n",
    "* Preprocesamiento de los datos, con el objetivo de remover los datos outliers. Descubrimos en el hito 1 que hay paises con datos demasiado alejados de la tendencia central, retroceso en datos acumulados (que debiesen ser estrictamente crecientes) y datos con ruido, como los del crucero Diamond Princess\n",
    "* Cálculo de coeficientes de modelo predictivo ARIMA, y su predicción para Chile\n",
    "* Clustering de los coeficientes\n",
    "\n",
    "###NOTA IMPORTANTE 1\n",
    "Para ejecutar este notebook, se necesitan scikit, matplotlib, numpy y statsmodels. Éste último para poder aplicar el modelo ARIMA. Se debe instalar con pip en el ambiente del libro:\n",
    "\n",
    "$ pip install statsmodels\n",
    "\n",
    "o con conda:\n",
    "\n",
    "$ conda install -c conda-forge statsmodels\n",
    "\n",
    "(https://www.statsmodels.org/stable/install.html)\n",
    "\n",
    "###NOTA IMPORTANTE 2\n",
    "Calcular el modelo ARIMA toma un tiempo de cómputo importante\n",
    "\n",
    "## Configuración base del notebook\n",
    "El bloque a continuación configura el libro, definiendo funciones y cargando las fuentes de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Librería de álgebra lineal\n",
    "import numpy as np \n",
    "# Procesamiento de datos y carga de archivos CSV\n",
    "import pandas as pd\n",
    "# Librearía gráfica\n",
    "import matplotlib.pylab as plt\n",
    "# Librería para usar recursos del sistema operativo\n",
    "import os \n",
    "# Librearía usada para extraer el nombre del archivo de cada path\n",
    "import ntpath\n",
    "# Librería gráfica\n",
    "import seaborn as sns\n",
    "# Librearía para usar herramientas relacionadas con\n",
    "# aprendizaje máquina. Se usó para preprocesar y\n",
    "# calcular regresión\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "# Colores\n",
    "from matplotlib import colors\n",
    "# Ticker\n",
    "import matplotlib.ticker as ticker\n",
    "# Animation\n",
    "import matplotlib.animation as animation\n",
    "# Arima\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Configuración general\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Matplotlib inline\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Se prepara carga de los archivos del dataset\n",
    "paths = []\n",
    "for dirname, _, filenames in os.walk('./novel-corona-virus-2019-dataset'):\n",
    "    for filename in filenames:\n",
    "        path = os.path.join(dirname, filename)\n",
    "        paths.append(path);\n",
    "dataFrames = {}\n",
    "for path in paths:\n",
    "    dataFrames[ntpath.basename(path)] = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga y reduce de datos por dia\n",
    "\n",
    "Se agrupan los atributos por indexados por fecha y país, para una manipulación más simple de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atributos_extra.correccion_nombres import cargar_correccion, atributos_extra\n",
    "covid19Data = dataFrames['covid_19_data.csv'].copy()\n",
    "dict_error, dict_pares = cargar_correccion()\n",
    "group_covid_data, paises_covid_df = atributos_extra(covid19Data, dict_error)\n",
    "group_covid_data.columns = group_covid_data.columns.droplevel(1)\n",
    "group_covid_data['ObservationDate'] = pd.to_datetime(group_covid_data['ObservationDate'])\n",
    "group_covid_data.set_index('Country/Region', inplace=True)\n",
    "group_covid_data.set_index('ObservationDate', inplace=True, append=True)\n",
    "group_covid_data.drop('Last Update', axis=1, inplace=True)\n",
    "# se limpia Laos por valores NaN\n",
    "group_covid_data.drop('Laos', level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paises_covid_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_covid_data.loc['Chile']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcular fenomeno por dia, basado en acumulados\n",
    "\n",
    "En el dataset vienen los acumulados de los contagios, fallecidos y recuperados. Se implementa un pequeño algoritmo que permite calcular los esos mismos atributos para cada día. Además, determinamos la fecha del primer contagio, de tal manera de tener una columna con el número de día desde el primer contagio, con el fin de poder comparar los países entre sí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def apply_primer_dia(data):\n",
    "    date_inicio_dataset = pd.to_datetime('22/01/2020') #datetime.date(2020,1,22)\n",
    "    #data = data.sort_index(ascending=True, level=0)\n",
    "    observation_date = data.index.get_level_values(1)\n",
    "    \n",
    "    num_dia_desde_primer_caso = 0\n",
    "    data[ 'num_dia_desde_22_enero'] = observation_date - date_inicio_dataset\n",
    "    \n",
    "    fecha_primer_caso = observation_date.min()\n",
    "    data['num_dia_desde_primer_caso'] = observation_date - fecha_primer_caso\n",
    "    # Set indexed\n",
    "    data.set_index('num_dia_desde_primer_caso', inplace=True, append=True)\n",
    "    # data.set_index('num_dia_desde_22_enero', inplace=True, append=True)\n",
    "    data.reset_index(level=1, inplace=True)\n",
    "    return data\n",
    "\n",
    "def compute_diff(col):\n",
    "    diff = col - col.shift(periods=1)\n",
    "    diff[0] = col[0]\n",
    "    # print(col[10])\n",
    "    # print(diff[9])\n",
    "    # print(20*'-')\n",
    "    return diff\n",
    "\n",
    "def apply_por_dia(data):\n",
    "    data['num_confirmados'] = compute_diff(data['Confirmed'])\n",
    "    data['num_recuperados'] = compute_diff(data['Recovered'])\n",
    "    data['num_fallecidos'] = compute_diff(data['Deaths'])\n",
    "    return data\n",
    "\n",
    "data = (\n",
    "    group_covid_data\n",
    "        .groupby('Country/Region')\n",
    "        .apply(lambda country: apply_primer_dia(country))\n",
    "        .droplevel(1)\n",
    "        .groupby(level=0)\n",
    "        .apply(lambda country: apply_por_dia(country))\n",
    ")\n",
    "\n",
    "data.loc['Chile'].sort_index(ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Como se comporta Chile en cantidad de confirmados?\n",
    "Se puede apreciar el \"pic\" en el momento en que el MISNAL corrigió los datos\n",
    "\n",
    "Además, se puede ver que había una tendencia a la baja que se rompe en los últimos días"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['Chile']['num_confirmados'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preproceso: Outliers\n",
    "Como parte del preprocesamiento, se realiza el siguiente tratamiento a los datos: Para cada país, se revisan las columnas de confirmados, fallecidos y recuperados. En cada una de ellas se buscan outliers, en este caso, valores que superen en magnitud al percentil 99 o que sean menores a cero. Estos se transorman a NaN y luego interpolamos los datos para llenar estos espacios.\n",
    "Además, se debe recalcular la cantidad acumulada por día para cada columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_outliers(dset_base, cols=['num_confirmados', 'num_fallecidos', 'num_recuperados']):  \n",
    "    for pais in dset_base['Country/Region'].unique():\n",
    "        for col in cols:\n",
    "            pais_col = dset_base[dset_base['Country/Region']==str(pais)][col]\n",
    "            changed_values = pd.Series(np.nan, index=pais_col.index)\n",
    "            for value in pais_col.iteritems():\n",
    "                if value[1] > pais_col.quantile(.99) or value[1] < 0:\n",
    "                    changed_values[value[0]] = np.nan\n",
    "                else:\n",
    "                    changed_values[value[0]] = value[1]\n",
    "            changed_values.interpolate(inplace=True)\n",
    "            acc = changed_values.cumsum()\n",
    "            dset_base.loc[(dset_base['Country/Region']==pais), col] = changed_values\n",
    "            dset_base.loc[(dset_base['Country/Region']==pais), str('acc') + col[3:]] = acc\n",
    "    return dset_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "def remove_outliers(col, q=0.99):\n",
    "    threshold = col.mean() + 3*col.std()\n",
    "    # threshold = col.quantile(q)\n",
    "    col[(col >= threshold)  | (col < 0)] = np.nan\n",
    "    # col[(col >= col.quantile(q)) | (col < 0)] = np.nan\n",
    "    #print(col.name, col.isna().sum())\n",
    "    res = col.interpolate()\n",
    "    return res\n",
    "\n",
    "def apply_remove_outliers(pais_col, cols=['num_confirmados', 'num_recuperados']):\n",
    "    # print(pais_col.name)\n",
    "    pais_col[cols] = pais_col[cols].apply(lambda x: remove_outliers(x))\n",
    "    # print(pais_col[cols].isna().sum())\n",
    "    return pais_col\n",
    "\n",
    "def compute_acumulados(pais_col):\n",
    "    pais_col['Confirmed'] = pais_col['num_confirmados'].cumsum()\n",
    "    pais_col['Recovered'] = pais_col['num_recuperados'].cumsum()\n",
    "    pais_col['Deaths'] = pais_col['num_fallecidos'].cumsum()\n",
    "    pais_col = pais_col.rename(\n",
    "        columns={\n",
    "            'Confirmed': 'acc_confirmados',\n",
    "            'Recovered': 'acc_recuperados',\n",
    "            'Deaths': 'acc_fallecidos'\n",
    "        }\n",
    "    )\n",
    "    return pais_col\n",
    "# data = data.drop('Papua New Guinea')\n",
    "res = (\n",
    "    data\n",
    "        .groupby(level=0)\n",
    "        .apply(lambda pais_col: apply_remove_outliers(pais_col))\n",
    "    )\n",
    "\n",
    "# Calculamos los acumulados\n",
    "res = (res\n",
    "    .groupby(level=0)\n",
    "    .apply(lambda pais_col: compute_acumulados(pais_col))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos limpios\n",
    "\n",
    "Ahora contamos con los datos limpios de anomalías estadísticas, agrupados por país y fecha, aprovechando la capacidad de los índices del tipo de datos DataFrame del módulo Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.loc['Chile'].sort_index(level=0, ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Como está Chile, en cantidad de confirmados, recuperados y fallecidos?\n",
    "\n",
    "Ahora, con los datos limpios, es posible ver la progresión de contagios, recuperados y fallecidos en Chile\n",
    "\n",
    "Por el momento, se podría pensar que lo pero ya pasó, sin embargo, en Europa los países han tenido rebrotes, como es el caso de Japón (ver figura a continuación a la de Chile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (14, 5)\n",
    "res.loc['Chile'][['num_confirmados', 'num_recuperados', 'num_fallecidos']].plot(figsize=figsize, grid=True, title='Fenomenos por día')\n",
    "res.loc['Chile'][['acc_confirmados', 'acc_recuperados', 'acc_fallecidos']].plot(figsize=figsize, grid=True, title='Fenomenos por dia, acumulado')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caso de rebrote en Japón, en el día 180, aproximadamente, desde el primer contagio\n",
    "figsize = (14, 5)\n",
    "res.loc['Japan'][['num_confirmados', 'num_recuperados', 'num_fallecidos']].plot(figsize=figsize, grid=True)\n",
    "res.loc['Japan'][['acc_confirmados', 'acc_recuperados', 'acc_fallecidos']].plot(figsize=figsize, grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Polinomial features\"\n",
    "\n",
    "Se procede a repetir el experimento del hito 1 con los datos actualizados. Nos gustaría demostrar que agrupar (cluster) mediante los coeficientes de ARIMA es mejor idea que con los polinomios, que fue lo que hicimos la primera oportunidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import pickle\n",
    "\n",
    "def load_model(pais):\n",
    "    filename_predictor = './models/polynomial_predictor_{}.joblib'.format(pais)\n",
    "    filename_features = './models/polynomial_features_{}.joblib'.format(pais)\n",
    "    print('Loading model {}'.format(filename_predictor))\n",
    "    print('Loading model {}'.format(filename_features))\n",
    "    return load_pickle(filename_predictor), load_pickle(filename_features)\n",
    "    \n",
    "\n",
    "def to_Xy(dset_pais, over='num_confirmados'):\n",
    "    X = np.linspace((0,),(len(dset_pais.index)-1,),len(dset_pais.index))\n",
    "    y = dset_pais[over].values\n",
    "    return X, y\n",
    "    \n",
    "def save_pickle(filename, model):\n",
    "    with open(filename,'wb') as outfile: \n",
    "        pickle.dump(model, outfile)\n",
    "        \n",
    "def load_pickle(filename):\n",
    "    with open(filename,'rb') as outfile:\n",
    "        return pickle.load(outfile)\n",
    "\n",
    "def apply_features(dset_pais, degree=6,save=False, pais_tosave=['Chile', 'USA', 'Japan'], over='num_confirmados'):\n",
    "    pais = dset_pais.index.get_level_values(0)[0]\n",
    "    X = np.linspace((0,),(len(dset_pais.index)-1,),len(dset_pais.index))\n",
    "    y = dset_pais[over].values\n",
    "\n",
    "    \n",
    "    poly_reg = PolynomialFeatures(degree=degree)\n",
    "    X_poly = poly_reg.fit_transform(X)\n",
    "    \n",
    "    pol_reg = LinearRegression()\n",
    "    pol_reg.fit(X_poly, y)\n",
    "\n",
    "    if save and pais in pais_tosave:\n",
    "        filename_predictor = './models/polynomial_predictor_{}.joblib'.format(pais)\n",
    "        filename_features = './models/polynomial_features_{}.joblib'.format(pais)\n",
    "        save_pickle(filename_predictor, pol_reg)\n",
    "        save_pickle(filename_features, poly_reg)\n",
    "\n",
    "    return pd.Series(poly_reg.transform(X)[-1])\n",
    "    \n",
    "def viz_polymonial(pol_reg, poly_reg, X):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.scatter(X, y, color='red')\n",
    "    plt.plot(X, pol_reg.predict(poly_reg.fit_transform(X)), color='blue')\n",
    "    plt.title('Regresión polinomial de los contagiados de Chile')\n",
    "    plt.xlabel('dias desde el contagio')\n",
    "    plt.ylabel('Contagiados acumulados')\n",
    "    plt.savefig('figures/chile_polinomial_reg.png')\n",
    "    plt.grid(True)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular features\n",
    "from src.clustering import TSNEVisualizer, ClusteringTunning\n",
    "over = 'num_confirmados'\n",
    "features = (\n",
    "    res\n",
    "        .groupby(level=0)\n",
    "        .apply(lambda dset_pais: apply_features(dset_pais, degree=7, save=True, over=over))\n",
    ")\n",
    "features.drop(0, axis=1, inplace=True)\n",
    "pais = 'Chile'\n",
    "chile_predictor, chile_features = load_model(pais)\n",
    "X, y = to_Xy(res.loc[pais], over=over)\n",
    "viz_polymonial(chile_predictor, chile_features, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.clustering import TSNEVisualizer, ClusteringTunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "raw_standard = StandardScaler().fit_transform(features.values)\n",
    "standard_feats = pd.DataFrame(raw_standard, columns = features.columns, index=features.index)\n",
    "standard_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el GMM\n",
    "visualizer = TSNEVisualizer()\n",
    "clustering = ClusteringTunning(visualizer).fit(standard_feats, n_components=2)\n",
    "prediction = clustering.predict(standard_feats)\n",
    "# clustering.scatter()\n",
    "clustering.estimator_\n",
    "\n",
    "# Plotear resultado del clustering\n",
    "clustering.plot_clustering(name='GMM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering.scatter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo del análisis de silueta\n",
    "\n",
    "El análsis de silueta nospermite demostrar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "print(silhouette_score(features, prediction.values.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo ARIMA\n",
    "Usamos un modelo ARIMA p=5,d=1,q=0. El detalle de estos parámetros se explica a continuación (basado en https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/)\n",
    "\n",
    "* El parámetro \"p\" es el número de observaciones pasadas que considera el modelo. También se le llama \"orden\".\n",
    "* El parámetro \"d\" es el grado de diferenciación. Esto se usa para generar estacionariedad en series no estacionarias\n",
    "* El parámetro \"q\" es el tamaño de la media móvil para el error de los datos anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se calculan coeficientes del ARIMA por país. Si ya está calculado se lee el csv\n",
    "# Los parámetros usados se escogieron en base a la moda, al calcular el óptimo para cada país\n",
    "dframe_arima = None\n",
    "if os.path.isfile('arima_coefs.csv') :\n",
    "    dframe_arima = pd.read_csv('arima_coefs.csv')\n",
    "else :\n",
    "    pais_arima_coef = dict()\n",
    "    paises_error = []\n",
    "    for pais in dset_base.pais.unique() :\n",
    "        dbset_actual = dset_base[dset_base['pais']==pais][['num_dia_desde_primer_caso','num_confirmados']]\n",
    "        dbset_actual= dbset_actual.reset_index()\n",
    "        series_actual = pd.Series(dbset_actual.num_confirmados.values, index=dbset_actual.num_dia_desde_primer_caso)\n",
    "        series_actual.index = pd.DatetimeIndex(series_actual.index).to_period('D')\n",
    "        try :\n",
    "            model = ARIMA(series_actual.astype(float), order=(5,1,0),missing=\"drop\")\n",
    "            model_fit = model.fit(disp=False)\n",
    "            pais_arima_coef[pais] = model_fit.params\n",
    "        except :\n",
    "            paises_error.append(pais)\n",
    "    dframe_arima = pd.DataFrame(data=pais_arima_coef)\n",
    "    dframe_arima.to_csv('arima_coefs.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coeficientes ARIMA\n",
    "\n",
    "Se muestra un resumen de los coeficientes calculados del modelo ARIMA por país"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe_arima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicción para Chile\n",
    "\n",
    "En el anexo está el cómo encontrar los parámetros óptimos para Chile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbset_actual = res.loc['Chile']['num_confirmados']\n",
    "dbset_actual= dbset_actual.reset_index()\n",
    "series_actual = pd.Series(dbset_actual.num_confirmados.values, index=dbset_actual.num_dia_desde_primer_caso)\n",
    "model = ARIMA(series_actual.astype(float), order=(6,0,0),missing=\"drop\")\n",
    "model_fit = model.fit(disp=False)\n",
    "model_fit.plot_predict(2,160);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de los resultados y discusión\n",
    "\n",
    "- Es importante contar con datos suficientemente limpios (bien estructurados y poco ruido). No es menor la cantidad de tiempo invertido en la limpieza tanto para el hito 1 como este hito.\n",
    "- El ruido de los datos se deriva principalmente del modo en que estos se han estado contabilizando por pais, por ejemplo: Para el caso de chile en la columan confirmados se presenta un peak que se eleva de 5000 a 35000 (aprox), este se debio a que el minsal no habia reportado una cantidad considerable de casos por considerarlos como 'pendientes'. La mayoria de los errores y ruidos en el dataset son de similar indole.\n",
    "- Es importante eliminar o suavizar lo mas posible el ruido debido a que al realizar clustering se podria estar agrupando por 'tipo de ruido' y o por tipo de curva o evolucion. Se sigue una logica similar respecto a ruido y modelamiento.\n",
    "- Se puede observar, en el gráfico de la predicción, que la cuarentena y el confinamiento han dado resultado y el país estaba bajando la tasa de contagio. Lamentablemente el gobierno está apresurando el regreso a \"la normalidad\", y podría suceder que haya un rebrote del virus\n",
    "- Se puede observar en el gráfico, en la sección del clustering, que la idea de \"clusterizar\" los coeficientes del modelo predictivo ARIMA no funcionó, como sí había funcionado con los coeficientes del polinomio de regresión del hito 1\n",
    "- Aún así, aprovechamos que tenemos el modelo ARIMA funcionando, y una búsqueda sus hiperparámetros para Chile (ver Anexo) para hacer una predicción para los próximos 20 días\n",
    "- No se encuentra evidencia significativa para afirmar que en efecto existen 'tipos' de curva o paises, coeficientes provenientes del modelo no parecieran tener una naturaleza adecuada para realizar esta tarea. Por otra parte, durante la revision de los clusters no se observa ningun patron distintivo, paises de cluster a y b (+++) no presentan caracteristicas diferenciales.\n",
    "\n",
    "Conclusiones:\n",
    "\n",
    "- Si bien el clustering no resulto ser efectivo es posible sacar algunas conclusiones a partir de esto: o bien los datos no son suficientemente trabajables (puede que no tenga sentido realizar clustering sobre ellos) o bien, puede ser que verdaderamente solo existe un tipo de tendencia y cada pais solo describira esta y no otra curva. Un motivo de porque esto es factible es la gran ineficiencia con la cual han actuado los distintos gobiernos, de modo que el qe solo exista un tipo de curva de avance no es algo tan extraño\n",
    "- El modelo pareciera ser suficientemente bueno para realizar predicciones a corto plazo (maximo 15-20)\n",
    "- Las técnicas de minería de datos permiten analizar el comportamiento de los datos y ayudar en la toma de desiciones. El Covid-19 ha generado grandes cambios en la sociedad, ha fallecido mucha gente y nos ha hecho cuestionar nuestra manera de actuar ante infecciones y problemas sanitarios. Proponemos que la minería de datos debiese ser parte de la política como soporte clave y permanente en la toma de desiciones estratégicas en los gobiernos actuales\n",
    "\n",
    "\n",
    "## Trabajo futuro\n",
    "\n",
    "Nos hubiese gustado haber podido seguir experimentando en las siguientes líneas:\n",
    "* Demostrar, empíricamente, si las medidas del gobierno, tanto el confinamiento y desconfinamiento, han sido apropiadas para dismunuir la tasa de contagios\n",
    "* Modelos predictivos basados en ARIMA para todos los países del mundo, y no sólo Chile\n",
    "* Aplicar ARIMA por intervalos de tiempo, donde podría mejorar la predicción\n",
    "\n",
    "## Anexo\n",
    "\n",
    "### Cálculo de hiper-parámetros del modelo ARIMA para Chile\n",
    "\n",
    "Basado en https://machinelearningmastery.com/grid-search-arima-hyperparameters-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# evaluate an ARIMA model for a given order (p,d,q)\n",
    "def evaluate_arima_model(X, arima_order):\n",
    "\t# prepare training dataset\n",
    "\ttrain_size = int(len(X) * 0.66)\n",
    "\ttrain, test = X[0:train_size], X[train_size:]\n",
    "\thistory = [x for x in train]\n",
    "\t# make predictions\n",
    "\tpredictions = list()\n",
    "\tfor t in range(len(test)):\n",
    "\t\tmodel = ARIMA(history, order=arima_order)\n",
    "\t\tmodel_fit = model.fit(disp=0)\n",
    "\t\tyhat = model_fit.forecast()[0]\n",
    "\t\tpredictions.append(yhat)\n",
    "\t\thistory.append(test[t])\n",
    "\t# calculate out of sample error\n",
    "\terror = mean_squared_error(test, predictions)\n",
    "\treturn error\n",
    " \n",
    "# evaluate combinations of p, d and q values for an ARIMA model\n",
    "def evaluate_models(dataset, p_values, d_values, q_values):\n",
    "\tdataset = dataset.astype('float32')\n",
    "\tbest_score, best_cfg = float(\"inf\"), None\n",
    "\tfor p in p_values:\n",
    "\t\tfor d in d_values:\n",
    "\t\t\tfor q in q_values:\n",
    "\t\t\t\torder = (p,d,q)\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tmse = evaluate_arima_model(dataset, order)\n",
    "\t\t\t\t\tif mse < best_score:\n",
    "\t\t\t\t\t\tbest_score, best_cfg = mse, order\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tcontinue\n",
    "\treturn {'best_cfg' : best_cfg, 'best_score' : best_score}\n",
    " \n",
    "\n",
    "# evaluate parameters\n",
    "p_values = range(4, 11)\n",
    "d_values = range(0, 3)\n",
    "q_values = range(0, 3)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "bests = dict()\n",
    "dbset_actual = res.loc['Chile']['num_confirmados']\n",
    "dbset_actual= dbset_actual.reset_index()\n",
    "series_actual = pd.Series(dbset_actual.num_confirmados.values, index=dbset_actual.num_dia_desde_primer_caso)\n",
    "print(evaluate_models(series_actual.values, p_values, d_values, q_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anexo II: Análisis de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.rename(columns={0: 'cluster_num'}, inplace=True)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paises_covid_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paises_covid_df.rename(columns={'pais': 'Country/Region'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paises_covid_df.set_index('Country/Region').merge(prediction, right_index=True, left_index=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values= prediction['cluster_num'].unique()\n",
    "print(values)\n",
    "#clustering=agglomerative_clusters ##estos son los valores que arroja el algoritmo \n",
    "#values= np.unique(agglomerative_clusters)\n",
    "\n",
    "\n",
    "#lo unico importante aqui es la ultima linea: df final debe tener todos los atributos, incluyendo la etiquieta de cluster \n",
    "data_agl = (\n",
    "    paises_covid_df\n",
    "        .set_index('Country/Region')\n",
    "        .merge(prediction, right_index=True, left_index=True)\n",
    "        .reset_index()\n",
    "        .copy()\n",
    "    )\n",
    "data_agl['cluster_num'] = prediction ####nombrar de igual forma la columna por fis <<cluster_num>>\n",
    "# data_agl = data_agl[['region', 'pais', 'cluster_num']]\n",
    "# data_agl_comp = pd.merge(data_agl, dset_extra, how='inner')\n",
    "# data_agl_comp = dset_base.merge(data_agl_comp.set_index('pais'), on='pais')\n",
    "# nombrar el dataset como data_cluster\n",
    "data_cluster=data_agl\n",
    "clustering='tipo de cluster' ##ver ejemplo (tomado del hito 1)\n",
    "\n",
    "interes=[ 'gdp_rank', 'gdp_usd', 'gdp_en_salud', 'edad_media', 'gini', 'poblacion', 'taza_muerte1000' ]\n",
    "colores= ['darkorange', 'springgreen', 'lightseagreen', 'orchid', 'chocolate', 'lightsteelblue', 'indianred', 'cadetblue', 'gold']\n",
    "\n",
    "datas={}\n",
    "for i in range(0, len(values)): #separamos y subimos al diccionario\n",
    "    llave=values[i]\n",
    "    datai = data_cluster.loc[data_cluster['cluster_num']==values[i]]\n",
    "    datas[llave]=datai\n",
    "\n",
    "    \n",
    "#estadisticas generales\n",
    "\"\"\"\n",
    "for i in range(0,len(values)):\n",
    "    print('info gnral cluster ', values[i], ':')\n",
    "    for a in range(0,len(interes)):\n",
    "        datos=datas[values[i]]\n",
    "        print(interes[a], 'promedio: ', datos[interes[a]].mean(), ';  desviacion:   ', datos[interes[a]].std() )\n",
    "        print('---------------------------------------')\n",
    "    print('*******************', 'fin cluster <', values[i] ,'>************************')\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#Paises en cluster\n",
    "for i in range(0,len(values)):\n",
    "    print('cluster', values[i])\n",
    "    data=datas[values[i]]\n",
    "    column_valxs = data[['Country/Region']].values\n",
    "    unique_values =  np.unique(column_valxs)    \n",
    "    print(unique_values)\n",
    "    print('-----------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "#Boxplots\n",
    "for col in interes:\n",
    "    par=col\n",
    "    data_cluster.groupby('cluster_num')\n",
    "    f, ax = plt.subplots(figsize=(7,5))\n",
    "    img=sns.boxplot(y=par, x= 'cluster_num', data=data_cluster, width=0.3, showfliers = True) #showf False no muestra outliers\n",
    "\n",
    "\n",
    "#densidades\n",
    "for col in interes:\n",
    "    par=col\n",
    "    plt.figure()\n",
    "    plt.title(col)\n",
    "    for i in range(0,len(datas)):\n",
    "        datos=datas[i]\n",
    "        datos=list(datos[col])\n",
    "        sns.distplot(datos, hist=False, color=colores[i])\n",
    "    plt.close\n",
    "\n",
    "\n",
    "#matrices corr atributos\n",
    "for i in range(0,len(values)):\n",
    "    print('para cluster:  ', values[i])\n",
    "    data=datas[values[i]]\n",
    "    # Incluir en la metodología\n",
    "    mm_scaler = preprocessing.MinMaxScaler()\n",
    "    data = data.drop([\"fecha_observacion\"  ,\"region_x\", \"region_y\",\"cluster_num\", 'Country/Region', \"num_dia_desde_22_enero\", \"num_dia_desde_primer_caso\" ],axis=1)\n",
    "    scaled_at = mm_scaler.fit_transform(data)\n",
    "    scaled = pd.DataFrame(scaled_at)\n",
    "    cormatrix = scaled.corr()\n",
    "    f, ax = plt.subplots()\n",
    "    sns.heatmap(cormatrix,vmax=1,vmin=-1,center=0, cmap='gnuplot2' )\n",
    "\n",
    "\n",
    "#Algunas curvas(por cluster)\n",
    "for i in values:\n",
    "    print('llave: ', i)\n",
    "    dato=datas[i]\n",
    "    column_valxs = dato[['Country/Region']].values\n",
    "    muestra = random.choices(np.unique(column_valxs), k=5)\n",
    "    k=0\n",
    "    var='num_dia_desde_primer_caso' #se puede hacer otro for o copiar el codigo cambiando esto para muertos y etc\n",
    "    plt.figure()\n",
    "    plt.title('cluster'+ str(i))\n",
    "    for k in range(0,len(muestra)):\n",
    "        pais=muestra[k]\n",
    "        print('pais: ', pais)\n",
    "        datos=data_cluster.loc[data_cluster['Country/Region']==pais]\n",
    "        sns.lineplot(x=var, y='acc_confirmados', data=datos, color=colores[k])\n",
    "    plt.close  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
